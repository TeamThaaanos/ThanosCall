from dotenv import load_dotenv
import os
import torch
from openai import OpenAI
from firebase_config import bucket, db
from transformers import BartForConditionalGeneration, PreTrainedTokenizerFast

# âœ… Firebase Storage & Cloud Functions í™˜ê²½ ê²½ë¡œ ì„¤ì •
MODEL_BUCKET = bucket
MODEL_PATH = "KOBARTmodel_finetuned"
LOCAL_MODEL_DIR = "/tmp/KOBARTmodel_finetuned"

# âœ… OpenAI API í‚¤ ì„¤ì •
load_dotenv()
openai_api_key = os.getenv("OPENAI_API_KEY")
if not openai_api_key:
    raise ValueError("OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
client = OpenAI(api_key=openai_api_key)

# ğŸ”¹ 1ï¸âƒ£ Firebase Storageì—ì„œ KoBART ëª¨ë¸ ë‹¤ìš´ë¡œë“œ
def download_kobart_model():
    """Firebase Storageì—ì„œ Fine-Tuned KoBART ëª¨ë¸ì„ ë‹¤ìš´ë¡œë“œí•˜ì—¬ ë¡œì»¬ì— ì €ì¥"""
    try:
        if not os.path.exists(LOCAL_MODEL_DIR):
            os.makedirs(LOCAL_MODEL_DIR)

        model_files = [
            "config.json",
            "generation_config.json",
            "model.safetensors",
            "special_tokens_map.json",
            "tokenizer_config.json",
            "tokenizer.json",
        ]

        for file in model_files:
            blob = bucket.blob(f"{MODEL_PATH}/{file}")
            local_path = os.path.join(LOCAL_MODEL_DIR, file)

            if not blob.exists():
                raise FileNotFoundError(f"Firebase Storageì—ì„œ {file}ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

            blob.download_to_filename(local_path)
            print(f"âœ… ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {file}")

        return LOCAL_MODEL_DIR
    except Exception as e:
        print(f"âŒ KoBART ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
        return None

# ğŸ”¹ 2ï¸âƒ£ KoBART ëª¨ë¸ì„ ë¡œë“œí•˜ëŠ” í•¨ìˆ˜
def load_kobart_model():
    """Fine-Tuned KoBART ëª¨ë¸ì„ ë¡œë“œ"""
    model_dir = download_kobart_model()
    
    if model_dir:
        try:
            tokenizer = PreTrainedTokenizerFast.from_pretrained(model_dir)
            model = BartForConditionalGeneration.from_pretrained(model_dir)
            return model, tokenizer
        except Exception as e:
            print(f"âŒ KoBART ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {str(e)}")

    return None, None  # KoBART ëª¨ë¸ì´ ì—†ì„ ê²½ìš°

# âœ… **í•œ ë²ˆë§Œ ëª¨ë¸ì„ ë¡œë“œí•˜ë„ë¡ ì „ì—­ ë³€ìˆ˜ ì„¤ì •**
MODEL, TOKENIZER = load_kobart_model()

# ğŸ”¹ 3ï¸âƒ£ Firebase Storageì—ì„œ ë³€í™˜ëœ í…ìŠ¤íŠ¸ íŒŒì¼ ê°€ì ¸ì˜¤ê¸°
def get_transcript_from_storage(consult_id):
    """Firebase Storageì—ì„œ ë³€í™˜ëœ í…ìŠ¤íŠ¸ íŒŒì¼ ê°€ì ¸ì˜¤ê¸°"""
    try:
        text_storage_path = f"transcripts/{consult_id}.txt"
        blob = bucket.blob(text_storage_path)

        if not blob.exists():
            raise FileNotFoundError(f"ë³€í™˜ëœ í…ìŠ¤íŠ¸ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {text_storage_path}")

        return blob.download_as_text()
    except Exception as e:
        raise Exception(f"í…ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {str(e)}")

# ğŸ”¹ 4ï¸âƒ£ KoBART ìš”ì•½ (ëª¨ë¸ì´ ìˆìœ¼ë©´ KoBART, ì—†ìœ¼ë©´ OpenAI GPT-3.5)
def summarize_text(transcript_text):
    """KoBART ëª¨ë¸ì„ ì´ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ ìš”ì•½, ëª¨ë¸ì´ ì—†ìœ¼ë©´ OpenAI API ì‚¬ìš©"""
    try:
        if MODEL and TOKENIZER:
            # âœ… KoBART ëª¨ë¸ ì‚¬ìš©
            input_text = transcript_text.strip().replace("\n", " ")
            inputs = TOKENIZER(input_text, return_tensors="pt", max_length=512, truncation=True)

            summary_ids = MODEL.generate(
                **inputs,
                max_length=256,
                min_length=80,
                num_beams=5,
                length_penalty=1.2,
                early_stopping=True
            )

            return TOKENIZER.decode(summary_ids[0], skip_special_tokens=True)
        else:
            # âœ… OpenAI GPT-3.5 ì‚¬ìš©
            print("âš ï¸ KoBART ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤. OpenAI APIë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
            return summarize_with_openai(transcript_text)
    except Exception as e:
        raise Exception(f"ìš”ì•½ ì‹¤íŒ¨: {str(e)}")

# ğŸ”¹ 5ï¸âƒ£ OpenAI GPT-3.5ë¥¼ ì´ìš©í•œ ìš”ì•½
def summarize_with_openai(transcript_text):
    """OpenAI GPT-3.5-turboë¥¼ ì´ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ ìš”ì•½"""
    try:
        response = client.chat.completions.create(
            model="gpt-4",
            messages=[
                {"role": "system", "content": "ë‹¹ì‹ ì€ í•œêµ­ì–´ ìš”ì•½ì„ ë„ì™€ì£¼ëŠ” AIì…ë‹ˆë‹¤."},
                {"role": "user", "content": f"ë‹¤ìŒ ë‚´ìš©ì„ 3ì¤„ë¡œ ìš”ì•½í•´ ì£¼ì„¸ìš”:\n{transcript_text}"}
            ]
        )
        summary = response.choices[0].message.content  # âœ… ì˜¬ë°”ë¥¸ ë°©ì‹ìœ¼ë¡œ content ê°€ì ¸ì˜¤ê¸°
        return summary
    except Exception as e:
        raise Exception(f"OpenAI ìš”ì•½ ì‹¤íŒ¨: {str(e)}")

# ğŸ”¹ 6ï¸âƒ£ ìš”ì•½ëœ í…ìŠ¤íŠ¸ë¥¼ Firebase Storageì— ì €ì¥
def save_summary_to_storage(consult_id, summary_text):
    """ìš”ì•½ëœ í…ìŠ¤íŠ¸ë¥¼ Firebase Storageì— ì €ì¥í•˜ê³  URL ë°˜í™˜"""
    try:
        summary_storage_path = f"summaries/{consult_id}.txt"
        summary_blob = bucket.blob(summary_storage_path)
        summary_blob.upload_from_string(summary_text, content_type="text/plain")

        summary_file_url = f"https://firebasestorage.googleapis.com/v0/b/{MODEL_BUCKET}/o/{summary_storage_path.replace('/', '%2F')}?alt=media"
        return summary_file_url
    except Exception as e:
        raise Exception(f"ìš”ì•½ ì €ì¥ ì‹¤íŒ¨: {str(e)}")

# ğŸ”¹ 7ï¸âƒ£ Firestoreì— ìš”ì•½ëœ í…ìŠ¤íŠ¸ ì •ë³´ ì €ì¥
def save_summary_to_firestore(consult_id, summary_file_url):
    """Firestoreì— ìš”ì•½ëœ í…ìŠ¤íŠ¸ íŒŒì¼ URL ì €ì¥"""
    try:
        doc_ref = db.collection("transcripts").document(consult_id)
        doc_ref.update({"summary_text_file_url": summary_file_url})
        return True
    except Exception as e:
        raise Exception(f"Firestore ìš”ì•½ ì €ì¥ ì‹¤íŒ¨: {str(e)}")

# ğŸ”¹ 8ï¸âƒ£ ì „ì²´ ìš”ì•½ ì‹¤í–‰ í•¨ìˆ˜
def process_summary(consult_id):
    """Firestoreì—ì„œ ë³€í™˜ëœ í…ìŠ¤íŠ¸ë¥¼ ê°€ì ¸ì™€ Fine-Tuned KoBART ëª¨ë¸ë¡œ ìš”ì•½ í›„ ì €ì¥"""
    try:
        transcript_text = get_transcript_from_storage(consult_id)

        # âœ… KoBART ë˜ëŠ” OpenAIë¥¼ ì´ìš©í•œ ìš”ì•½
        summary_text = summarize_text(transcript_text)

        # âœ… Firebase Storageì— ì €ì¥
        summary_file_url = save_summary_to_storage(consult_id, summary_text)

        # âœ… Firestoreì— ì €ì¥
        save_summary_to_firestore(consult_id, summary_file_url)

        return {
            "message": "ìš”ì•½ ì €ì¥ ì„±ê³µ",
            "summary_text": summary_text,
            "summary_file_url": summary_file_url
        }
    except Exception as e:
        raise Exception(f"ìš”ì•½ ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}")
